{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m2RXTtet4lm7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pyspark\n",
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "from io import StringIO \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType \n",
    "from pyspark.sql.types import ArrayType, DoubleType, BooleanType\n",
    "from pyspark.sql.functions import col,array_contains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando a SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m2RXTtet4lm7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "conf = pyspark.SparkConf()\n",
    "\n",
    "# Criando uma sessão com o Spark que existe localmente(atualmente configurado junto com o JupyterLab)\n",
    "conf.setMaster(\"local[1]\") \n",
    "conf.set(\"spark.driver.host\", \"awari-jupyterlab\") \\\n",
    "    .set(\"spark.sql.sources.commitProtocolClass\", \"org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol\") \\\n",
    "    .set(\"parquet.enable.summary-metadata\", \"false\") \\\n",
    "    .set(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\") \\\n",
    "    .set(\"spark.driver.port\", \"20020\") \\\n",
    "    .set(\"spark.hadoop.fs.s3a.endpoint\", 'awari-nginx:9000') \\\n",
    "    .set(\"spark.hadoop.fs.s3a.endpoint.region\", 'sa-east-1') \\\n",
    "    .set(\"spark.hadoop.fs.s3a.access.key\", 'XKQmEpTesp9sCSQG') \\\n",
    "    .set(\"spark.hadoop.fs.s3a.secret.key\", 'ba4xzjLWC6zEC52jEVJw3oc3gvuEpkzX') \\\n",
    "    .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .set(\"spark.hadoop.com.amazonaws.services.s3.enableV2\", \"true\") \\\n",
    "    .set(\"spark.hadoop.fs.s3a.committer.staging.conflict-mode\", \"replace\") \\\n",
    "    .set(\"spark.hadoop.fs.s3a.fast.upload\", True) \\\n",
    "    .set(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "    .set(\"spark.hadoop.fs.s3a.committer.name\", \"directory\") \\\n",
    "    .set(\"spark.hadoop.fs.s3a.committer.staging.tmp.path\", \"/tmp/staging\")\n",
    "\n",
    "conf.setAppName('AwariAula08-S33')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando o cliente para conectar ao Minio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = boto3.client('s3', \n",
    "    endpoint_url='http://awari-minio-nginx:9000',\n",
    "    aws_access_key_id='XKQmEpTesp9sCSQG', #substitua por sua access key id\n",
    "    aws_secret_access_key='ba4xzjLWC6zEC52jEVJw3oc3gvuEpkzX', #substitua por sua secret access key\n",
    "    aws_session_token=None,\n",
    "    config=boto3.session.Config(signature_version='s3v4'),\n",
    "    verify=False,\n",
    "    region_name='sa-east-1'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurando o diretório e lendo os arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dir_path = \"./arquivos/\"\n",
    "csv_files = [file for file in os.listdir(dir_path) if file.endswith('.csv')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando os CSVs da pasta e salvando como arquivos JSON no MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for file in csv_files:\n",
    "    path = os.path.join(dir_path, file)\n",
    "    \n",
    "    # Lendo o arquivo CSV\n",
    "    df = spark.read.csv(path, header=True, inferSchema=True)\n",
    "    \n",
    "    # Salvando no formato JSON localmente\n",
    "    local_output_dir = \"/tmp/\" + file.replace(\".csv\", \".json\")\n",
    "    df.write.mode(\"overwrite\").json(local_output_dir)\n",
    "    \n",
    "    # Identificando os arquivos JSON dentro do diretório\n",
    "    json_files = [f for f in os.listdir(local_output_dir) if f.endswith('.json')]\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        json_file_path = os.path.join(local_output_dir, json_file)\n",
    "        s3_path = f'tarefa/json/{file.replace(\".csv\", \"\")}/{json_file}'\n",
    "        \n",
    "        # Fazendo upload do arquivo JSON para o MinIO\n",
    "        with open(json_file_path, 'rb') as f:\n",
    "            client.upload_fileobj(f, 'aula-08', s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando os JSONs e salvando como arquivos CSV no MinIO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "bucket_name = 'aula-08'\n",
    "directory_name = 'tarefa'\n",
    "\n",
    "# Liste todos os diretórios/objetos no bucket 'aula-08'\n",
    "objects = client.list_objects(Bucket=bucket_name)['Contents']\n",
    "\n",
    "# Filtrando os diretórios sob 'tarefa/json'\n",
    "json_directories = [obj['Key'].split('/')[2] for obj in objects if obj['Key'].startswith(f\"{directory_name}/json/\")]\n",
    "json_directories = list(set(json_directories))  # Removendo duplicatas\n",
    "\n",
    "for json_dir in json_directories:\n",
    "    # Caminho para os arquivos JSON no diretório atual\n",
    "    s3_json_path = f\"s3a://{bucket_name}/{directory_name}/json/{json_dir}/*.json\"\n",
    "    \n",
    "    # Lendo todos os arquivos JSON dentro do diretório atual\n",
    "    try:\n",
    "        df = spark.read.json(s3_json_path)\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    # Caminho de saída local para o arquivo CSV\n",
    "    local_dir_path = f\"./tmp/{json_dir}\"\n",
    "    try:\n",
    "        df.coalesce(1).write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(local_dir_path)\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    # Buscar o arquivo CSV gerado dentro do diretório\n",
    "    csv_file = glob.glob(f\"{local_dir_path}/part-*.csv\")[0]\n",
    "    \n",
    "    # Subindo para o MinIO\n",
    "    s3_path = f\"{directory_name}/csv/{json_dir}/output.csv\"\n",
    "    try:\n",
    "        with open(csv_file, 'rb') as f:\n",
    "            client.upload_fileobj(f, bucket_name, s3_path)\n",
    "    except:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando os CSVs e salvando como arquivo .parquet no MinIO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Definindo os diretórios CSV para os estados e municípios\n",
    "csv_directories = {\n",
    "    \"estados\": f\"s3a://{bucket_name}/{directory_name}/csv/estados/output.csv\",\n",
    "    \"municipios\": f\"s3a://{bucket_name}/{directory_name}/csv/municipios/output.csv\"\n",
    "}\n",
    "\n",
    "# Caminho de saída no MinIO para arquivos parquet\n",
    "parquet_paths = {\n",
    "    \"estados\": f\"s3a://{bucket_name}/{directory_name}/parquet/estados\",\n",
    "    \"municipios\": f\"s3a://{bucket_name}/{directory_name}/parquet/municipios\"\n",
    "}\n",
    "\n",
    "# Lendo CSVs e salvando como Parquet\n",
    "for key, csv_path in csv_directories.items():\n",
    "    # Lendo CSV\n",
    "    df = spark.read.option(\"header\", True).csv(csv_path)\n",
    "    \n",
    "    # Salvando como Parquet no MinIO\n",
    "    df.write.mode(\"overwrite\").parquet(parquet_paths[key])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Importando os CSVs e salvando como tabelas em um banco de dados PostgreSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "# Configurações de conexão ao PostgreSQL\n",
    "host = \"127.0.0.1\"\n",
    "port = \"5432\"  # Atualizado para a porta correta\n",
    "user = \"postgres\"\n",
    "password = \"postgres\"\n",
    "database = \"postgres\"  # Conecta-se primeiro ao banco padrão para criar um novo\n",
    "\n",
    "# Conecta ao banco de dados\n",
    "try:\n",
    "    conn = psycopg2.connect(dbname=database, user=user, password=password, host=host, port=port)\n",
    "    conn.autocommit = True  # Habilita o autocommit para permitir a criação do banco de dados\n",
    "\n",
    "    # Cria um novo banco de dados chamado aula-08\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(\"CREATE DATABASE aula_08\")\n",
    "    print(\"Banco de dados 'aula-08' criado com sucesso!\")\n",
    "except psycopg2.errors.DuplicateDatabase:\n",
    "    print(\"Banco de dados 'aula-08' já existe.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao criar o banco de dados: {e}\")\n",
    "finally:\n",
    "    if 'conn' in locals():  # Adicionada verificação se 'conn' existe\n",
    "        conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "db_url = \"jdbc:postgresql://awari-database:5432/aula-08\"\n",
    "\n",
    "# Lendo CSVs e salvando no PostgreSQL\n",
    "for key, csv_path in csv_directories.items():\n",
    "    # Lendo CSV\n",
    "    df = spark.read.option(\"header\", True).csv(csv_path)\n",
    "    \n",
    "    # Escrevendo no PostgreSQL\n",
    "    df.write.jdbc(url=db_url, table=key, mode=\"overwrite\", properties=db_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAH7iNcj4PEE"
   },
   "source": [
    "---\n",
    "\n",
    "Notebook utilizado para fins educacionais da **Awari**.\n",
    "\n",
    "**© AWARI. Todos os direitos reservados.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMpmB1/ZiTM76Kc2DEBlVuc",
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "1WPJzzm-0hWc6Q5JetcdpWbJrsPesiWLs",
     "timestamp": 1666129304753
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
