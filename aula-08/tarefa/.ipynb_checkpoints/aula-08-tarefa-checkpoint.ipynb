{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "m2RXTtet4lm7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/spark-3.3.1-bin-hadoop3/python (3.3.1)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.5)\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.10/site-packages (1.28.35)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3) (0.6.2)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: botocore<1.32.0,>=1.31.35 in /opt/conda/lib/python3.10/site-packages (from boto3) (1.31.35)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.32.0,>=1.31.35->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<1.32.0,>=1.31.35->boto3) (1.26.14)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.32.0,>=1.31.35->boto3) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "from io import StringIO \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType \n",
    "from pyspark.sql.types import ArrayType, DoubleType, BooleanType\n",
    "from pyspark.sql.functions import col,array_contains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando a SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "m2RXTtet4lm7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "conf = pyspark.SparkConf()\n",
    "\n",
    "# Criando uma sessão com o Spark que existe localmente(atualmente configurado junto com o JupyterLab)\n",
    "conf.setMaster(\"local[1]\") \n",
    "conf.set(\"spark.driver.host\", \"awari-jupyterlab\") \\\n",
    "    .set(\"spark.sql.sources.commitProtocolClass\", \"org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol\") \\\n",
    "    .set(\"parquet.enable.summary-metadata\", \"false\") \\\n",
    "    .set(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\") \\\n",
    "    .set(\"spark.driver.port\", \"20020\") \\\n",
    "    .set(\"spark.hadoop.fs.s3a.endpoint\", 'awari-nginx:9000') \\\n",
    "    .set(\"spark.hadoop.fs.s3a.endpoint.region\", 'sa-east-1') \\\n",
    "    .set(\"spark.hadoop.fs.s3a.access.key\", 'XKQmEpTesp9sCSQG') \\\n",
    "    .set(\"spark.hadoop.fs.s3a.secret.key\", 'ba4xzjLWC6zEC52jEVJw3oc3gvuEpkzX') \\\n",
    "    .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .set(\"spark.hadoop.com.amazonaws.services.s3.enableV2\", \"true\") \\\n",
    "    .set(\"spark.hadoop.fs.s3a.committer.staging.conflict-mode\", \"replace\") \\\n",
    "    .set(\"spark.hadoop.fs.s3a.fast.upload\", True) \\\n",
    "    .set(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "    .set(\"spark.hadoop.fs.s3a.committer.name\", \"directory\") \\\n",
    "    .set(\"spark.hadoop.fs.s3a.committer.staging.tmp.path\", \"/tmp/staging\")\n",
    "\n",
    "conf.setAppName('AwariAula08-S33')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando o cliente para conectar ao Minio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = boto3.client('s3', \n",
    "    endpoint_url='http://awari-minio-nginx:9000',\n",
    "    aws_access_key_id='XKQmEpTesp9sCSQG', #substitua por sua access key id\n",
    "    aws_secret_access_key='ba4xzjLWC6zEC52jEVJw3oc3gvuEpkzX', #substitua por sua secret access key\n",
    "    aws_session_token=None,\n",
    "    config=boto3.session.Config(signature_version='s3v4'),\n",
    "    verify=False,\n",
    "    region_name='sa-east-1'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurando o diretório e lendo os arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dir_path = \"./arquivos/\"\n",
    "csv_files = [file for file in os.listdir(dir_path) if file.endswith('.csv')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando os CSVs da pasta e salvando como arquivos JSON no MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for file in csv_files:\n",
    "    path = os.path.join(dir_path, file)\n",
    "    \n",
    "    # Lendo o arquivo CSV\n",
    "    df = spark.read.csv(path, header=True, inferSchema=True)\n",
    "    \n",
    "    # Salvando no formato JSON no MinIO\n",
    "    output_path = \"s3a://aula-08/tarefa/\" + file.replace(\".csv\", \".json\")\n",
    "    df.write.json(output_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando os JSONs e salvando como arquivos CSV no MinIO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lista todos os arquivos JSON no bucket aula-08 na pasta tarefa/\n",
    "json_files = client.list_objects_v2(Bucket='aula-08', Prefix='tarefa/')['Contents']\n",
    "\n",
    "for file_info in json_files:\n",
    "    file_key = file_info['Key']\n",
    "    if file_key.endswith('.json'):\n",
    "        # Lê o arquivo JSON do MinIO usando Spark\n",
    "        df = spark.read.json(f\"s3a://aula-08/{file_key}\")\n",
    "        \n",
    "        # Defina a chave para o novo arquivo CSV\n",
    "        csv_key = file_key.replace(\".json\", \".csv\")\n",
    "        \n",
    "        # Converta o DataFrame para CSV e salve no MinIO\n",
    "        csv_buffer = StringIO()\n",
    "        df.write.csv(csv_buffer)\n",
    "        client.put_object(Body=csv_buffer.getvalue(), Bucket='aula-08', Key=csv_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando os CSVs e salvando como arquivo .parquet no MinIO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando um contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAH7iNcj4PEE"
   },
   "source": [
    "---\n",
    "\n",
    "Notebook utilizado para fins educacionais da **Awari**.\n",
    "\n",
    "**© AWARI. Todos os direitos reservados.**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMpmB1/ZiTM76Kc2DEBlVuc",
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "1WPJzzm-0hWc6Q5JetcdpWbJrsPesiWLs",
     "timestamp": 1666129304753
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
